{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision¶\n",
    "### Module 5. Deep Learning for Computer Vision\n",
    "#### Task 5:\n",
    "    1. Paper review\n",
    "    2. CNN visualization\n",
    "    3. Experiment summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Paper review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title**: STEP: Spatio-Temporal Progressive Learning for Video Action Detection\n",
    "\n",
    "**Authors**: Yang, Xitong and Yang, Xiaodong and Liu, Ming-Yu and Xiao, Fanyi and Davis, Larry S. and Kautz, Jan\n",
    "\n",
    "**Link**: http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_STEP_Spatio-Temporal_Progressive_Learning_for_Video_Action_Detection_CVPR_2019_paper.pdf\n",
    "\n",
    "**Tags**: computer vision, deep learning, action detection\n",
    "\n",
    "**Year**: 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: The authors offer a progressive learning framework for spatio-temporal action detection in video - **Spatio-TEmporalProgressive(STEP) action detector**. Unlike existing methods that directly perform action detection in one run, the framework involves a multi-step optimization process that progressively refines the initial proposals towards the final solution. STEP consists of 2 stages: *spatial refinement* and *temporal extension*, where the former starts from sparse initial proposals and iteratively updates bounding boxes, and the latter gradually and adaptively increases sequence length to incorporate more related temporal context. STEP more effectively makes use of longer temporal information by handling the spatial displacement problem in action tubes (a sequence of bounding boxes of action). Extensive experiments on two benchmarks show that STEP consistently brings performance gains by usingonly a handful of proposals and a few updating steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How**:STEP approach performs action detection at clip level, i.e., detection results are first obtained from each clip and then linked to build action tubes across a whole video. It is assumed that each action tubelet of a clip has a constant action label, considering the short duration of a clip, e.g., within one second. The target is to tackle the action detection problem through a few progressive steps, rather than directly detecting actions all at one run. \n",
    "\n",
    "In order to detect the actions in a clip $I_t$ with $K$ frames, according to the maximum progressive steps $S_{max}$, we first extract the convolutional features for a set of clips $I={I_{t−Smax+1}, ..., I_t, ..., I_{t+Smax−1}}$ using a backbone network such as VGG16 or I3D. The progressive learning starts with $M$ predefined proposal cuboids $B^{0}=\\{b^0_i\\}^{M}_{i=1}$ and $b^0_i∈R^{K×4}$, which are sparsely sampled from a coarse-scale grid of boxes and replicated across time to form the initial proposals. These initial proposals are then progressively updated to better classify and localize the actions. At each steps, proposals are updated by performing the following processes in order:\n",
    "\n",
    "• Extend: the proposals are temporally extended to the adjacent clips to include longer-range temporal context, and the temporal extension is adaptive to the movement of actions\n",
    "    \n",
    "• Refine: the extended proposals are forwarded to the spatial refinement, which outputs the classification and regression results\n",
    "\n",
    "• Update: all proposals are updated using a simple greedy algorithm, i.e., each proposal is replaced by the regression output with the highest classification score: \n",
    "\n",
    "$b^s_i.=l^s_i(c*)$,  $c*= arg max p^s_i(c)$\n",
    "\n",
    "where $c$ is an action class, $p^s_i∈ R^{(C+1)}$ is the probability distribution of the $i$th proposal over $C$ action classes plus background, $l^s_i∈ R^{K×4×C}$ denotes its parameterized coordinates (for computing the localization loss) ateach frame for each class, and $.=$ indicates decoding the parameterized coordinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "Algorithm: STEP Action Detection for Clip $I_t$\n",
    "_____________________\n",
    "**Input**: video clips $I$, initial proposals $B^0$, and maximum steps $S_{max}$\n",
    "\n",
    "**Output**:\n",
    "detection results $\\{(p^{S_{max}}_i,l^{S_{max}}_i)\\}^{M}_{i=1}$\n",
    "\n",
    "    1 extract convolutional features for video clips I\n",
    "    2 for s←1 to S_max do \n",
    "    3    if s==1 then:\n",
    "    4        //initial proposals\n",
    "    5        B'{s-1} ← B_0\n",
    "    6    else:\n",
    "    7        //temporal extension\n",
    "    8        B'{s-1} ← Extend(B{s-1})\n",
    "    9    end\n",
    "    10   // spatial refinement\n",
    "    11   {(p{S}_i,l^{S}_i)\\}^{M}_{i=1} ← Refine(B'{s-1})\n",
    "    12   //update proposals\n",
    "    13   B{s} ← Update({(p{S}_i,l^{S}_i)\\}^{M}_{i=1})\n",
    "    14 end\n",
    "--------------------\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**: \n",
    "1. Comparison with the state-of-the-art methods on **UCF101** by frame-mAP and video-mAP under different IoU thresholds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method   |      frame-mAP@0.5    |  video-mAP@0.05\t |  video-mAP@0.1\t |  video-mAP@0.2\t |\n",
    "|:----------:|:-------------:|------:|------:|------:|\n",
    "| MR-TS  |  65.7 | 78.8 | 77.3 | 72.9 |\n",
    "| ROAD  |    --   |   -- | -- | 73.5 |\n",
    "| CPLA | -- |    79.0 | 77.3 | 73.5 |\n",
    "| RTPR  | -- |    81.5 | 80.7 | 76.3 |\n",
    "| PntMatch | 67.0 |    79.4 | 77.7 | 76.2 |\n",
    "| T-CNN | 67.3 |    78.2 | 77.9 | 73.1 |\n",
    "| ACT | 69.5 |    -- | -- | 76.5 |\n",
    "| **STEP** | **75.0** |    **84.6** | **83.1** | **76.6** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Comparison with the state-of-the-art methods on **AVA** by frame-mAP under IoU@0.5. “*” means the results obtained by incorporating optical flow\n",
    "\n",
    "| Method   |      frame-mAP|\n",
    "|:----------:|:-------------:|\n",
    "| Single Frame*  |  14.2 |\n",
    "| I3D  |    14.7   |\n",
    "| I3D* | 15.6 |\n",
    "| ACRN*  | 17.4 |\n",
    "| **STEP** | **18.6** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. STEP runs at 21 fps using early fusion with 11 initial proposals and 3 steps on a single GPU, which is com-parable with the clip based approach (23 fps) and muchfaster than the frame based method (4 fps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CNN visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of 2-branch network\n",
    "![alt text](img/CV_module5_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP: progressive learning framework structure\n",
    "![alt text](img/CV_module5_2.jpg)\n",
    "\n",
    "**Notations**:\n",
    "\n",
    "**T** - temporal extension\n",
    "\n",
    "**S** - spatial refinement\n",
    "\n",
    "**C** - classification\n",
    "\n",
    "**L** - localization (L0 - initial proposals)\n",
    "\n",
    "**number** - step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Experiment summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series of experiments were done based on a3_cifar10.ipynb (https://github.com/lyubonko/ucu2019/blob/master/assignments/a3_cifar10.ipynb\n",
    ") using CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The initial model given in the notebook has the following structure:\n",
    "$[conv-relu-pool]x2-[fc-relu]x3 $  \n",
    "with the initial parameters:\n",
    "        \n",
    "        n_epoch = 5\n",
    "        batch_size = 4\n",
    "        lr=0.01\n",
    "        momentum = 0.9\n",
    "        optimizer = SGD+momentum\n",
    "        loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to tune the parameters, however the obtained results in most cases were much worse than initial ones (seems the author spent enough time to pick-up such a splendit model). The intermediate results are shown bellow:\n",
    "\n",
    "| model  | n_epoch| learning_rate |  batch_size | optimizer  |batch_norm|  train_accuracy | test_accuracy|comments|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5| 0.001  |  4 |  SGD+momentum |  - |  - |  63 |initial settings |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3 |10 | 0.001  |  4 |  SGD+momentum |  - |  - |  65 |  -|\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.01 | 4  | SGD+momentum  | - |  -  |  27 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.0005 | 4  | SGD+momentum  | -  |  - |  31 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.001 | 4  | Adam  | -|  -   |  47 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.001 | 4  | SGD+momentum  | +|  -   |  60 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.01 | 4  | SGD+momentum  | +|  -   |  60 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.01 | 64  | SGD+momentum  | +|  -   |  **66** | best obtained result on 5-epoch-train|\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.001 | 64  | SGD+momentum  | +|  -   |  50 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.1 | 64  | SGD+momentum  | +|  -   |  49 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |5|  0.01 | 64  | SGD+momentum  |-|  -   |  63 | -  |\n",
    "| [conv-relu-pool]x2-[fc-relu]x3  |50|  0.01 | 64  | SGD+momentum  |+|  -   |  46 | -  |\n",
    "\n",
    "After these series of experiments it could be proved that batch normalization positively influences accuracy (however, the batch_size should be big enough to see this changes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Having spent more then 10 hours only on parameters tunning using the original version of the notebook, I decided to rewrite a code a bit to be able to train the models using cuda in colab and check the accuracy on train and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Assuming that parameter tunning would never provide with desirable \"95% accuracy\":D on cifar10 dataset, I decided to play with the famous CNN architecture such as VGG (16, 19), DenseNet and newly baked EfficientNet. The results are provided bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model|\tn_epoch|\tlearning_rate|\tbatch_size|\toptimizer|\tbatch_norm|\ttrain_accuracy|\ttest_accuracy|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|efficientnet|5|\t0.1|\t32|\tSGD+momentum\t|**+**\t|55|\t57|\n",
    "|efficientnet|5|\t0.01|\t32|\tSGD+momentum\t|**+**\t|74|\t73|\n",
    "|efficientnet|5|\t0.001|\t32|\tSGD+momentum\t|**+**\t|70|\t68|\n",
    "|vgg16|\t5|\t0.1|\t32|\tSGD+momentum\t|**+**\t|64|\t62|\n",
    "|vgg16|\t5|\t0.01|\t32|\tSGD+momentum\t|**+**\t|82|\t80|\n",
    "|vgg16|\t5|\t0.001|\t32|\tSGD+momentum\t|**+**\t|88|\t79|\n",
    "|vgg16|\t5|\t0.1|\t**64**|\tSGD+momentum\t|**+**|\t**85**|\t**82**|\n",
    "|vgg19|\t5|\t0.1|\t32|SGD+momentum\t\t|**+**\t|52\t|48|\n",
    "|vgg19|\t5|\t0.01|\t32|\tSGD+momentum\t|**+**\t|78\t|78|\n",
    "|vgg19|\t5|\t0.001|\t32|\tSGD+momentum\t|**+**\t|87|\t80|\n",
    "|densenet|\t5|\t0.01|\t64|\tSGD+momentum\t|**+**|\t85|\t81|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To proceed with training on more n_epoch VGG16 was chosen, due to the speed and relatively one of the best performances after training on 5 epoch. (The same result was obtained for trained NN of Densenet architecture, however, the training was more time consuming). Here is the visualization of the training:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img/vgg16_iter.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule of learning rate was done using native torch function (learning rate starts with 0.01 value, and after 100 epoch, it decreseas to 0.001)\n",
    "        \n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dependencies above we can see that having learning_rate = 0.01 the accuracy on the training dataset would come to plato of 84%. Since the trick with learning rate schedule was applied while traning NN, the it dramatically increased by 4 %. Thus the best obtained result of accuracy on test dataset is 88% (VGG16-architecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. TODO (future): \n",
    "\n",
    "        1) play around scheduling decresing learning rate over time \n",
    "        2) add data augmentation\n",
    "        3) check bigger batch size when batch norm applied\n",
    "        4) use pretrained model\n",
    "        5) check adviced in notebook architectures - to build a better feeling of which blockes has more influence on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to repo with experimental notebooks: https://github.com/kasprova/CV_UCU/tree/master/module5/practice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
